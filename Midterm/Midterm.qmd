---
title: "Predicting Home Sale Price in Philadelphia"
subtitle: "MUSA 508 Midterm"
author: "Akira Di Sandro and Nissim Lebovits"
date: today
format: 
  html:
    embed-resources: true
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
editor: source
execute:
  warning: false
  error: false
  messages: false
  echo: true
  cache: false
---

## Summary

## Introduction

Housing market price estimators like Zillow's Zestimate can give a general idea of a house's value, but more often than not, professional appraisers and real estate agents do no recommend relying on these tools as they tend to have high error rates. In addition to inconveniencing a home buyer or seller, the high error rates of automated valuation models (AVM; e.g. Zillow's Zestimate) may misrepresent low-income and majority-minority, especially Black-majority neighborhoods where home price value errors are higher compared to majority-white neighborhoods (cite https://www.urban.org/sites/default/files/publication/103429/how-automated-valuation-models-can-disproportionately-affect-majority-black-neighborhoods_1.pdf). As well, a home's value is often used to determine a homeowner's mortgage loan approval and to inform policymakers on the general state of the housing market (cite https://www.iaao.org/media/standards/Standard_on_Automated_Valuation_Models.pdf). Inaccurate housing estimates may lead to misinformed housing and economic policies disproportionately affecting non-white neighborhoods, reifying racial and class disparities in housing.

There are many reasons why these price estimators may be inaccurate (e.g. data availability, inability to capture temporal housing market patterns, etc.), and among them lack of local intelligence stands out. Each urban area has its own culture and unique characteristics that need to be factored into AVMs in order to more accurately predict housing prices. As residents and spatial analysts of the city of Philadelphia, our team was able to apply our knowledge of the city and its neighborhoods to the creation of an improved prediction model. 

Even with a deeper understanding of Philadelphia, creating an improved AVM for the city is still a challenging task. All kinds of data are available including health and human services, transportation, public safety, parks and recreation, and food in addition to the traditional housing market and attribute data used in AVMs. Though all of these data are interrelated and inform each other and home price, limiting the amount of predictors to only keep those that are most informative can be a daunting task. We often create new features by redefining (sometimes by combining) variables that improve our prediction of home price. Luckily we implement technology that makes this decision-making process easier. 

Another possible challenge when it comes to constructing an AVM is data availability, or rather unavailability. Some neighborhoods, especially low-income neighborhoods and regions where the population does not trust in government, data that we use in our AVMs may be missing for various reasons. Some of the variables that are most informative to a model may be resources that are unavailable in low-income neighborhoods. On the other hand, even if the resources are available and utilized in the community, a region with high distrust in government may not report information in the government Census and other data surveys, resulting in missing data. Ultimately, if a model relies on variables that are missing in neighborhoods with certain characteristics, it will predict home price with more error in these regions.

Keeping these potential pitfalls in mind, our overall modeling strategy was as follows. We first gathered open-source data about Philadelphia to examine and create variables about Philadelphia houses and neighborhoods that can be predictors in our model. We then input a selection of predictors into an ordinary least squares (OLS) regression, specifically an OLS step-wise regression, to narrow down and determine the combination of the most informative predictors. After assessing our model fit by looking at mean absolute error (MAE) and mean absolute percentage error (MAPE), we revised our selection of predictors and repeated the process until we were happy with our model.

Our newly improved model uses xx independent variables to predict housing prices in Philadelphia and is able to do so with a MAE of xx and MAPE of xx. The most informative predictors in our OLS regression are var1, var2, and var3. We observed clustering in home price predictions with a Moran's *I* of 0.615 (p < 0.001) meaning __. 

```{r setup}
library(tidyverse)
library(olsrr)
library(sf)
library(caret) # add dummy vars
library(tmap)
library(fastDummies)
library(tidycensus)
library(spdep)
library(sfdep)
library(curl)
library(zip)
library(rsgeo)
library(janitor)
library(spatstat)
library(maptools)
library(terra)
library(ggthemr)

tmap_mode('view')
options(tigris_use_cache = TRUE, scipen = 999)
ggthemr('flat')

crs <- "epsg:2272"

set.seed(42)

# load functions from functions.R
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```
## Methods

### Data Sources

TO DO: DESCRIBE HOW DATA WAS GATHERED

We pulled data from OpenData arcGIS, house attribute data provided by Zillow (studentData), Philadelphia neighborhood data (credit azavea github user), and ACS Census data.

### Data Wrangling
```{r data wrangle}
#| warning: false

phl_path <- "https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson"
phl <- st_read(phl_path, quiet = TRUE) %>%
          st_transform(crs = crs)

### base data-----------------------------------------------
data_path <- ("data/2023/studentData.geojson")
data <- st_read(data_path, quiet = TRUE)

drop <- c("objectid", "assessment_date", "beginning_point", "book_and_page", "category_code", 
          "cross_reference", "date_exterior_condition", "house_number", "location", "owner_1", 
          "owner_2", "parcel_number", "recording_date", "registry_number", "sale_date",
          "mailing_address_1", "mailing_address_2", "mailing_care_of", "mailing_zip", "mailing_street", 
          "mailing_city_state", "building_code", "geographic_ward", "state_code", "street_code", 
          "street_name", "street_designation", "street_direction", "census_tract", "suffix",
          "zip_code", "building_code_new", "year_built_estimate", "pin", "toPredict", "unit", "exempt_land",
          "building_code_description"
          )

to_cat <- c("category_code_description", "garage_type")

data <- data %>%
          mutate(non_resident_owner = mailing_address_1 == mailing_street) %>%
          select(-drop) %>%
          mutate_at(to_cat, as.character) %>% 
          # mutate(house_extension = as.numeric(house_extension)) %>% # for some reason, this *really* fucks up the model
          st_transform(crs = crs) %>%
          filter(sale_price > 1) %>% #need to filter for realistic sale prices, per https://www.phila.gov/media/20220525080608/tax-year-2023-mass-appraisal-valuation-methodology.pdf
          mutate(number_of_rooms = ifelse(is.na(number_of_rooms), number_of_bedrooms + number_of_bathrooms, number_of_rooms))


data <- data[phl, ]
keep_columns <- sapply(data, function(col) length(unique(col)) > 1) #drop columns with only one unique value (i.e., no variance)
data <- data[, keep_columns]

# as.integer(sqrt(length(unique(data$building_code_description_new))))
# 
# data %>%
#   group_by(building_code_description_new) %>%
#   summarize(avg_sale_price = mean(sale_price)) %>%
#   ggplot() +
#   geom_col(aes(x = reorder(building_code_description_new, avg_sale_price), y = avg_sale_price)) +
#   theme(axis.text.x = element_text(hjust = 1, angle = 45))

# over 800,000
# over 600,000
# 0er 400,000
# over 200,000
# under 200,000

avg_sale_prices_x_building_type <- data %>%
  group_by(building_code_description_new) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  mutate(building_type_price_class = case_when(
    avg_sale_price > 800000 ~ "most expensive",
    avg_sale_price <= 800000 & avg_sale_price > 600000 ~ "more expensive",
    avg_sale_price <= 600000 & avg_sale_price > 400000 ~ "expensive",
    avg_sale_price <= 400000 & avg_sale_price > 200000 ~ "less expensive",
    avg_sale_price <= 200000 ~ "least expensive",
  )) %>%
  select(building_code_description_new, building_type_price_class) %>%
  st_drop_geometry()

data <- left_join(data, avg_sale_prices_x_building_type, by = "building_code_description_new")


# data %>%
#   group_by(quality_grade) %>%
#   summarize(avg_price = mean(sale_price)) %>%
#   arrange(desc(avg_price)) %>%
#   ggplot() +
#   geom_col(aes(x = reorder(quality_grade, avg_price),
#                y = avg_price))

data <- data %>%
          mutate(quality_grade = case_when(
            quality_grade == "X" ~ "Highest",
            quality_grade %in% c("X-", "A+", "A", "A-") ~ "High",
            quality_grade %in% c("C", 'D', "D-", "D+", "6", "D-", "E", "C-", "E-", "E+") ~ "Lowest",
            TRUE ~ "Mid"
          ))


### neighborhoods-----------------------------------------------
hoods_path <- 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'
hoods <- st_read(hoods_path, quiet = T) %>%
  st_transform(crs = crs) %>%
  select(mapname)

data <- st_join(data, hoods)

# 
# as.integer(sqrt(length(unique(data$mapname))))
# 
# data %>%
#   group_by(mapname) %>%
#   summarize(avg_sale_price = mean(sale_price)) %>%
#   arrange(desc(avg_sale_price)) %>%
#   ggplot() +
#   geom_histogram(aes(x = avg_sale_price), bins = 12) +
#   scale_x_continuous(breaks = c(50000, 100000, 250000, 500000, 1000000))

avg_sale_prices_x_hood <- data %>%
  group_by(mapname) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  mutate(hood_price_class = case_when(
    avg_sale_price > 750000 ~ "most expensive",
    avg_sale_price <= 750000 & avg_sale_price > 500000 ~ "more expensive",
    avg_sale_price <= 500000 & avg_sale_price > 250000 ~ "expensive",
    avg_sale_price <= 250000 & avg_sale_price > 100000 ~ "less expensive",
    avg_sale_price <= 100000 ~ "least expensive",
  )) %>%
  select(mapname, hood_price_class) %>%
  st_drop_geometry()

data <- left_join(data, avg_sale_prices_x_hood, by = "mapname")


### ACS-----------------------------------------------
phl_acs <- readRDS("phl_acs.RData")
phl_acs <- phl_acs %>%
              select(-c(
                medRent,
                pctPov
              ))
data <- st_join(data, phl_acs)

### tree canopy cover-----------------------------------------------
# url <- "https://national-tes-data-share.s3.amazonaws.com/national_tes_share/pa.zip.zip"
# tmp_file <- tempfile(fileext = ".zip")
# curl_download(url, tmp_file)
# 
# unzipped_folder_1 <- tempfile()
# unzip(tmp_file, exdir = unzipped_folder_1)
# shp_files <- list.files(unzipped_folder_1, pattern = "\\.shp$", recursive = TRUE, full.names = TRUE)
# tree_canopy_gap <- st_read(shp_files[1], quiet = TRUE)  # assuming there's only one .shp file
# phl_tree_canopy <- tree_canopy_gap %>%
#   filter(state == "PA",
#          county == "Philadelphia County") %>%
#   transmute(tree_cover = 1 - tc_gap) %>%
#   st_transform(crs = crs)
# 
# data <- st_join(data, phl_tree_canopy)

### spatial lag of sale price-----------------------------------------------
data <- data %>% 
          mutate(nb = st_knn(geometry, k = 15),
                 wt = st_weights(nb),
                 price_lag = st_lag(sale_price, nb, wt)) %>%
          select(-c(nb, wt))

### proximity to commmercial corridors-----------------------------------------------
corridors_path <- "https://opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson"
corridors <- st_read(corridors_path, quiet= TRUE) %>% st_transform(crs = crs)

nearest_fts <- sf::st_nearest_feature(data, corridors)

# convert to rsgeo geometries
x <- rsgeo::as_rsgeo(data)
y <- rsgeo::as_rsgeo(corridors)

# calculate distance
data$dist_to_commerce <- rsgeo::distance_euclidean_pairwise(x, y[nearest_fts])

### proximity to downtown-----------------------------------------------
# downtown <- st_sfc(st_point(c(-75.16408, 39.95266)), crs = 4326)
# downtown_sf <- st_sf(geometry = downtown)
# downtown_sf <- downtown_sf %>% st_transform(crs= crs)
# 
# nearest_fts <- sf::st_nearest_feature(data, downtown_sf)
# 
# # convert to rsgeo geometries
# x <- rsgeo::as_rsgeo(data)
# y <- rsgeo::as_rsgeo(downtown_sf)
# 
# # calculate distance
# data$dist_to_downtown <- rsgeo::distance_euclidean_pairwise(x, y[nearest_fts])

### shootings density-----------------------------------------------
# shootings_url <- 'https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+shootings+WHERE+year+%3E+2022&filename=shootings&format=geojson&skipfields=cartodb_id'
# shootings <- st_read(shootings_url, quiet = TRUE) %>% 
#                   st_transform(crs = crs) %>%
#                   filter(!st_is_empty(geometry))
# 
# sp_points <- as(shootings, "Spatial")
# ppp_object <- as.ppp(sp_points)
# kde <- densityAdaptiveKernel(ppp_object)
# kde_spatraster <- rast(kde)
# 
# guncrime <- terra::extract(kde_spatraster, data) %>%
#               transmute(ID = ID,
#                         guncrime_density = scale(lyr.1))
# 
# data <- cbind(data, guncrime) 


  
```

TO DO: Present a table of summary statistics with variable descriptions. Sort these variables by their category (internal characteristics, amenities/public services or spatial structure). Check out the `stargazer` package for this.


TO DO: Present a correlation matrix


TO DO: Present 4 home price correlation scatterplots that you think are of interest. I’m going to look for interesting open data that you’ve integrated with the home sale observations.

```{r 4 interesting scatters}
# below is just a template of the code
# st_drop_geometry(studentData) %>% 
#     dplyr::select(sale_price,total_livable_area, year_built, total_area) %>%
#     filter(year_built > 0, total_livable_area > 0, total_area > 0,
#            sale_price <= 4000000) %>% # for now look at places < $4mil
#     mutate(pct_livable = total_livable_area/total_area) %>% 
#     gather(Variable, Value, -sale_price) %>% 
#     ggplot(aes(Value, sale_price)) +
#     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
#     facet_wrap(~Variable, ncol = 2, scales = "free") +
#     labs(title = "Price as a function of continuous variables") +
#     plotTheme()
```


TO DO: Develop 3 maps of 3 of your most interesting independent variables.



#### Map of Home Sale Prices in Philadelphia

```{r ppsqft map}
# create price per squre foot variable
ppsqft <- data %>% dplyr::select(sale_price,total_area) %>% 
      drop_na() %>% 
      mutate(ppsqft_m = ifelse(sale_price != 0 & total_area != 0 & total_area != 1,sale_price/total_area, NA)) %>%  # there are some places where total_area == 1
      filter(!is.na(ppsqft_m)) # dropping missing ppsqft_m values for a cleaner map

mypalette1 <- colorRampPalette(c("#fcb39f","#7a0728"))(5)

ggplot() +
  geom_sf(data = phl_acs, color = "white") +
  geom_sf(data = ppsqft, aes(colour = q5(ppsqft_m)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = mypalette1,
                      labels=qBr(ppsqft,"ppsqft_m"),
                      name=bquote((USD/ft^2))) +
  labs(title = "Home Sale Price in Philadelphia",
       subtitle="Price Per Square Foot", # make sure to note that NAs were dropped for visualization purpose
       caption = "Figure xx") +
  mapTheme()
```



### OLS Model
```{r ols model} 


# data %>%
#   group_by(mapname) %>%
#   summarize(avg_sale_price = mean(sale_price)) %>%
#   arrange(desc(avg_sale_price)) %>%
#   head()

# m <- cor(numeric_only %>% na.omit())
# corrplot::corrplot(m, na.action = na.exclude)

# numeric_only <- data %>% st_drop_geometry() %>% select(where(is.numeric))
# ols_step_best_subset(model)
# model <- lm(sale_price ~ ., data = numeric_only)
# aic_preds <- ols_step_both_aic(model)$predictors
keep_vars <- c("price_lag",
               "garage_spaces",
               "total_livable_area",
               "medHHInc",
               "number_of_rooms",
               "number_of_bathrooms",        
               "depth",
               "off_street_open",
               "fireplaces",
               "interior_condition",
               "number_stories",
               "pctWhite",
               "number_of_bedrooms",
               "exterior_condition",
               "dist_to_commerce",
               "year_built",
               "totPop",
               "total_area",
                "sale_price", 
               "hood_price_class", 
               "building_type_price_class", 
               "quality_grade",
               "musaID")


dummied_data <- data %>%
                  select(all_of(keep_vars)) %>% 
                  dummy_cols(select_columns = c("hood_price_class",
                                                "building_type_price_class",
                                                "quality_grade")) %>%
                    clean_names() %>%
                    select(-c(hood_price_class,
                              building_type_price_class,
                              quality_grade)) %>%
                    na.omit()

reg_data <- dummied_data %>%
              st_drop_geometry() %>%
              select(-c(geometry,
                        musa_id))


# processed_dd <- preProcess(dummied_data %>% select(-sale_price),
#                            method = c("center", "scale", "YeoJohnson", "nzv"))
# 
# processed_dd
# predict(processed_dd, newdata = dummied_data %>% select(-sale_price))
# 
# processed_dd$method$remove

# 
# model <- lm(sale_price ~ ., data = dummied_data)
# stepwise_selection <- ols_step_both_aic(model)
# keep_vars <- c(stepwise_selection$predictors, "sale_price")
# 
# final_data <- dummied_data %>% select(keep_vars) %>% st_drop_geometry()


customSummary <- function(data, lev = NULL, model = NULL) {
  mpe <- mean((data$obs - data$pred) / data$obs) * 100
  mae <- mean(abs(data$obs - data$pred))
  rmse <- sqrt(mean((data$obs - data$pred)^2))
  rsq <- cor(data$obs, data$pred)^2
  out <- c(MAE = mae, RMSE = rmse, Rsquared = rsq, MPE = mpe)
  out
}

train_control <- trainControl(method = "cv", # using leave one out instead of regular cv, so gives better esimate of error
                              number = 10,
                              summaryFunction = customSummary)


model <- train(sale_price ~ ., 
               data = reg_data,
               trControl = train_control,
               method = "lm",
               na.action = na.exclude)

# print(model)

model_for_plot <- lm(sale_price ~ ., 
               data = reg_data)
ols_plot_resid_fit(model_for_plot)
# can we do this in ggplot? mostly for aesthetic purposes but also to add Figure xx caption
```

## Results

### Regression Results

TO DO: Provide a polished table of your training set lm summary results (coefficients, R2 etc).


TO DO: Provide a polished table of mean absolute error and MAPE for a single test set.


TO DO: Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. (already have the histogram of MAE w 100 folds) 

```{r mae hist}
ggplot(data = model$resample) +
  ggplot2::geom_histogram(aes(x = MAE)) +
  labs(title = "Distribution of MAE",
       subtitle = "K-Fold Cross Validation; k = 100",
       x = "Mean Absolute Error",
       y = "Count",
       caption = "Figure xx")
```


TO DO: Plot predicted prices as a function of observed prices


TO DO: Provide a map of your residuals for your test set. Include a Moran’s I test and a plot of the spatial lag in errors.


TO DO: Provide a map of your predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE”


TO DO: Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood. (code chunk reids is close to what we want, but use MAPE instead of resids)


TO DO: Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.


TO DO: Using tidycensus, split your study area into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?


### Spatial Autocorrelation and Error
```{r autocorr}
data_nb <- data %>% 
          mutate(nb = st_knn(geometry, k = 5),
                 wt = st_weights(nb),
                 .before = 1)


# calculate global moran's i for sale_price
global_moran(data_nb$sale_price, data_nb$nb, data_nb$wt)$`I`

# moran monte carlo
moranMC = global_moran_perm(data_nb$sale_price, data_nb$nb, data_nb$wt, alternative = "two.sided", 999)
moranMC

moranMCres = moranMC$res |>
  as.data.frame()

colnames(moranMCres) = "col1"

ggplot(moranMCres) +
  geom_histogram(aes(col1), bins = 100) +
  geom_vline(xintercept = moranMC$statistic, col = "red") +
  labs(title = "Histogram of MoranMCres",
       x = "moranMCres",
       y = "Frequency",
       caption = "Figure xx") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


spdep::moran.plot(data_nb$sale_price, nb2listw(data_nb$nb),
           xlab = "Sale Price", 
           ylab = "Spatial Lag")
```

```{r resids}
reg <- lm(sale_price ~ ., 
               data = reg_data,
          na.action = na.exclude)

resids_data <- cbind(reg$model, 
                  reg$residuals, 
                  reg$fitted.values,
                  dummied_data$musa_id,
                  dummied_data$geometry) %>%
              mutate(resids = `reg$residuals`,
                     pred_value = `reg$fitted.values`) %>%
            st_sf()

resids_data <- st_join(resids_data, hoods)

error_x_hood <- resids_data %>%
                  group_by(mapname) %>%
                  summarize(avg_error = mean(abs(resids))) %>%
                  arrange(desc(avg_error)) %>%
                  select(avg_error, mapname) %>%
                  st_drop_geometry() %>%
                  left_join(., hoods, by = "mapname") %>%
                  st_sf()

tmap_mode('plot')

tm_shape(error_x_hood) +
  tm_polygons(col = 'avg_error', border.col = NA, style = 'jenks')


# reg_data <- inner_join()
# reg_data$residuals <- reg$residuals
# 
# reg_data <- final_data %>%
#               transmute(
#                stand_resids = rstandard(reg),
#                      spatial_lag = st_lag(stand_resids, 
#                                              nb, 
#                                              weights)
#               )
```

```{r lisa map}
lisa = local_moran(data_nb$sale_price, data_nb$nb, data_nb$wt, nsim = 999)
data_nb <- cbind(data_nb, lisa)

lisa_sample <- sample_n(data_nb, 100)

tmap_mode('view')

tm_shape(lisa_sample) +
  tm_dots(col = "p_ii")
```

## Discussion

Is this an effective model? What were some of the more interesting variables?  How much of the variation in prices could you predict? Describe the more important features? Describe the error in your predictions?  According to your maps, could you account the spatial variation in prices?  Where did the model predict particularly well? Poorly? Why do you think this might be?

### Accuracy and Generalizability

## Conclusion

Would you recommend your model to Zillow? Why or why not? How might you improve this model?

