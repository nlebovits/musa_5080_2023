---
title: "Geospatial Risk Prediction: Illegal Dumping"
subtitle: "MUSA 508, Lab #3"
author: "Nissim Lebovits"
date: today
format: 
  html:
    embed-resources: true
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
    number-sections: true
editor: source
execute:
  warning: false
  error: false
  messages: false
  echo: true
  cache: false
---

## Summary
```{r setup, set.seed(33)}

library(tidyverse)
library(sf)
library(rphl)
library(tidycensus)
library(sfdep)
library(tmap)
library(ggpubr)
library(ggthemr)
library(arcpullr)
library(caret)
library(rsample) 
library(tidymodels)
library(recipes)
library(poissonreg)
library(spatialsample)

source("themes.R")

knitr::opts_chunk$set(cache = T)
options(tigris_use_cache = TRUE, scipen = 999)

ggthemr('flat')

tmap_options(basemaps = "Esri.WorldGrayCanvas") #set global tmap basemap

crs <- "epsg:2272"

base_url = "https://phl.carto.com/api/v2/sql"

three_years_ago = (lubridate::ymd(Sys.Date()) - lubridate::years(3))
one_year_ago = (lubridate::ymd(Sys.Date()) - lubridate::years(1))

#### DEFINE FUNCTIONS


### 311 complaints query---------------------------------------------------------
get_complaints <- function(query, base_url, service_names_list){
  
  complaints <- st_as_sf(get_carto(query,
                              format = 'csv',
                              base_url = base_url,
                              stringsAsFactors = FALSE) %>%
                      dplyr::filter(service_name %in% service_names_list,
                             !is.na(lat),
                             !is.na(lon)),
                      coords = c("lon", "lat"),
                      crs = st_crs('EPSG:4326')) %>%
                      mutate(requested_datetime = as.Date(requested_datetime),
                             closed_datetime = as.Date(closed_datetime)) %>%
                      st_transform(crs = st_crs("EPSG:2272")) # will need these to be projected for KDE later
}
```

```{r wrangleR}

### PHL bounds and grid----------------------------------------
phl_path <- "https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson"
phl <- st_read(phl_path, quiet = TRUE) %>%
          st_transform(crs = crs)

cellsize <- 2000

phl_grid <- st_make_grid(phl, crs = crs, cellsize = cellsize, square = FALSE) %>% 
                st_as_sf()

phl_grid <- phl_grid[phl, ]
phl_grid <- phl_grid %>%
                mutate(grid_id = 1:n())


### neighborhoods-----------------------------------------------
hoods_path <- 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'
hoods <- st_read(hoods_path, quiet = T) %>%
  st_transform(crs = crs) %>%
  select(mapname)

### illegal dumping----------------------------------------
# dumping_query <- sprintf("
#                   SELECT *
#                   FROM public_cases_fc
#                   WHERE requested_datetime >= '%s' AND requested_datetime < '%s'
#                  ", three_years_ago, one_year_ago)
# 
# dumping_strings <- c("Illegal Dumping")  
# complaints <- get_complaints(dumping_query, base_url, dumping_strings)
# 
# complaints <- complaints |>
#   mutate(
#     closed_datetime = ifelse(closed_datetime == "", NA, closed_datetime),
#     response_time_days = case_when(
#       is.na(closed_datetime) ~ as.numeric(difftime(Sys.Date(), requested_datetime, units = "days")),
#       TRUE ~ as.numeric(difftime(
#         closed_datetime, requested_datetime, units = "days"
#       ))
#     ),
#     count_complaints = 1
#   )

complaints_path <- "hw3_data/complaints.geojson"
# saveRDS(complaints, complaints_path)
complaints <- readRDS(complaints_path)

complaints_sample <- slice_sample(complaints, prop = .05)

# complaints to grid----------------------------------------
dumping_net <- st_join(phl_grid, complaints) %>%
                  group_by(grid_id) %>%
                  summarize(count_complaints = sum(count_complaints),
                            count_complaints = ifelse(is.na(count_complaints), 0, count_complaints))


### vacant properties ----------------------------------------
# vacant_points_path <- "https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/Vacant_Indicators_Points/FeatureServer/0/"
# vacant_points <- get_spatial_layer(vacant_points_path) %>% st_transform(crs = crs) %>% mutate(count_vacant = 1)

vacant_points_path <- "hw3_data/vacant_points.geojson"
# saveRDS(vacant_points, vacant_points_path)
vacant_points <- readRDS(vacant_points_path)

vacancy_net <- st_join(phl_grid, vacant_points) %>%
                  group_by(grid_id) %>%
                  summarize(count_vacant = sum(count_vacant),
                            count_vacant = ifelse(is.na(count_vacant), 0, count_vacant)) %>%
                  st_drop_geometry()

full_net <- left_join(dumping_net, vacancy_net) %>% st_as_sf()

### abandoned cars----------------------------------------
# abandoned_auto_list <- c("Abandoned Vehicle")
# abandoned_cars <- get_complaints(dumping_query, base_url, abandoned_auto_list) %>% mutate(cars_count = 1)

abandoned_cars_path <- "hw3_data/abandoned_cars.geojson"
# saveRDS(abandoned_cars, abandoned_cars_path)
abandoned_cars <- readRDS(abandoned_cars_path)

cars_net <- st_join(phl_grid, abandoned_cars) %>%
                  group_by(grid_id) %>%
                  summarize(cars_count = sum(cars_count),
                            cars_count = ifelse(is.na(cars_count), 0, cars_count)) %>%
                  st_drop_geometry()

full_net <- left_join(full_net, cars_net) %>% st_as_sf()

### street lights out----------------------------------------
# lights_list <- c("Alley Light Outage", "Street Light Outage")
# lights_out <- get_complaints(dumping_query, base_url, lights_list) %>% mutate(outage_count = 1)

lights_out_path <- "hw3_data/lights_out.geojson"
# saveRDS(lights_out, lights_out_path)
lights_out <- readRDS(lights_out_path)

lights_net <- st_join(phl_grid, lights_out ) %>%
                  group_by(grid_id) %>%
                  summarize(outage_count = sum(outage_count),
                            outage_count = ifelse(is.na(outage_count), 0, outage_count))

full_net <- left_join(full_net, st_drop_geometry(lights_net), by = "grid_id") %>% st_as_sf()

### building permits----------------------------------------
# building_perms_query <- sprintf("
#                   SELECT permitissuedate, typeofwork, ST_Y(the_geom) AS lat, ST_X(the_geom) AS lng
#                   FROM permits
#                   WHERE permitissuedate >= '%s' AND permitissuedate < '%s'
#                  ", three_years_ago, one_year_ago)
# 
# building_permits <- st_as_sf(get_carto(building_perms_query,
#                               format = 'csv',
#                               base_url = base_url,
#                               stringsAsFactors = FALSE) |>
#                       dplyr::filter(
#                              !is.na(lat),
#                              !is.na(lng)),
#                       coords = c("lng", "lat"),
#                       crs = st_crs('EPSG:4326')) |>
#                       st_transform(crs = st_crs("EPSG:2272")) %>%
#                     mutate(permits_count = 1)

building_permits_path <- "hw3_data/building_permits.geojson"
# saveRDS(building_permits, building_permits_path)
building_permits <- readRDS(building_permits_path)

permits_net <- st_join(phl_grid, building_permits) %>%
                  group_by(grid_id) %>%
                  summarize(permits_count= sum(permits_count),
                            permits_count = ifelse(is.na(permits_count), 0, permits_count))

full_net <- left_join(full_net, st_drop_geometry(permits_net), by = "grid_id") %>% st_as_sf()


### income----------------------------------------

### race----------------------------------------


### add hoods for grouping--------------------------
intersection <- st_intersection(phl_grid, hoods)
intersection$intersection_area <- st_area(intersection)
max_intersection <- intersection %>%
  group_by(grid_id) %>%
  filter(intersection_area == max(intersection_area)) %>%
  ungroup() %>%
  select(grid_id, mapname) %>%
  st_drop_geometry()

grid_w_hood <- left_join(phl_grid, max_intersection, by = c("grid_id" = "grid_id")) %>%
                st_drop_geometry()


full_net <- left_join(full_net, grid_w_hood, by = "grid_id")
```



## Introduction
Illegal dumping is a major issue in Philadelphia. Especially in low-income, minority neighborhoods, illegal dumping has a significant impact on quality of life, property values, safety, and public health. For years, the City has failed to address the issue. This is likely due to a combination of factors such as bureaucratic ineptitude, lack of resources, the COVID-19 pandemic, and the sheer scale of the problem (there are, as of this writing, more than 1,600 open cases across the city, but only one detective assigned to solve them). In the case of illegal dumping, *selection bias* (when inaccurate sampling negatively impacts the quality of data) are likely a major issue. Residents of wealthier neighborhoods are more likely to report illegal dumping for a variety of reasons, such as more trust in City services and a higher expectation that something will actually be done about the problem. Residents of poorer, majority-minority neighborhoods are less inclined to report illegal dumping cases in part because they feel that nothing will be done about the problem anyway. As a result, there is likely under-reporting of illegal dumping in the neighborhoods it most impacts.

```{r map dumping}
#| label: complaints-dot-map
#| fig-cap: "A map of 5% of dumping complaints, 2020-22"

tmap_mode('plot')

tm <- tm_shape(phl) +
  tm_borders() +
tm_shape(complaints_sample) +
  tm_dots(clustering = TRUE)

tmap_theme(tm, "Sample of Complaints\n2020-22")
```

```{r dumping hist}
#| label: dumping-hist
#| fig-cap: "The distribution of illegal dumping across Philadelphia hexagon bins, 2020-22"

n_bins <- as.integer(sqrt(nrow(phl_grid)))

ggplot(full_net) +
  geom_histogram(aes(x = count_complaints), bins = n_bins) +
  labs(title = "Distribution of Illegal Dumping Complaints",
       subtitle = "per Fishnet Grid Cell",
       x = "Complaints",
       y = "Count")
```

In order to more successfully intervene *proactively* to mitigate illegal dumping, it is useful to have a model that accounts for this selection bias. Below, we use a cross-validated Poisson regression to build a model that attempts to do exactly that. By incorporating associated risk factors (in this case, vacant properties, abandoned cars, street lights out, building permits, income, and race), we seek to construct a model that more can more accurately predict future occurrences of illegal dumping while mitigating the influence of selection bias.

## Methods

### Data Sources & Cleaning


A note on data: data are drawn from 2020 to 2022, the peak years of the COVID-19 pandemic. As a result, they are likely anomalous in many ways, such as the fact that the Kenney administration cut the sanitation department's budget.

#### The Fishnet Grid

```{r complaints net}
#| label: dumping-net
#| fig-cap: "The distribution of illegal dumping across Philadelphia, 2020-22"

tmap_mode('plot')

tm <- tm_shape(dumping_net) +
  tm_polygons(col = "count_complaints", border.alpha = 0, style = "fisher", palette = "viridis")

tmap_theme(tm, "Fishnet Grid of Illegal Dumping Distribution\n2020-22")
```

Using local moran's I calculations, we can identify hotspots of illegal dumping in Philadelphia.
```{r lisa}
dumping_net <- dumping_net %>%
                  mutate(nb = st_contiguity(x),
                         wt = st_weights(nb),
                         dumping_lag = st_lag(count_complaints, nb, wt))

lisa <- dumping_net %>% 
  mutate(moran = local_moran(count_complaints, nb, wt)) %>% 
  tidyr::unnest(moran) %>% 
  mutate(pysal = ifelse(p_folded_sim <= 0.1, as.character(pysal), NA))

palette <- c("High-High" = "#B20016", 
             "Low-Low" = "#1C4769", 
             "Low-High" = "#24975E", 
             "High-Low" = "#EACA97")

tm <- tm_shape(lisa) +
  tm_polygons(col = "pysal", border.alpha = 0, style = "cat", palette = palette, textNA = "Not a Hotspot")

tmap_theme(tm, "Illegal Dumping Hotspots\n2020-22")
```

#### Correlations
```{r corrplots}

ggscatter(full_net, x = "count_complaints", y = "count_vacant",
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   ) + stat_cor(method = "pearson", label.x = 75, label.y = 75)

ggscatter(full_net, x = "count_complaints", y = "cars_count",
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   ) + stat_cor(method = "pearson", label.x = 75, label.y = 75)

ggscatter(full_net, x = "count_complaints", y = "permits_count",
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   ) + stat_cor(method = "pearson", label.x = 75, label.y = 75)

ggscatter(full_net, x = "count_complaints", y = "outage_count",
   add = "reg.line",  # Add regressin line
   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
   conf.int = TRUE # Add confidence interval
   ) + stat_cor(method = "pearson", label.x = 75, label.y = 75)
```

### Model
Found that a regression tree had a lower MAE than a GLM, but oh, well.

### Poisson Regression

## Results

```{r consistent}
reg_vars <- c("count_vacant",
              "cars_count",
              "outage_count",
              "permits_count",
              "count_complaints",
              "mapname")

reg_data <- full_net %>% 
              dplyr::select(all_of(reg_vars)) %>% 
              st_point_on_surface() # %>% 
             #  mutate(across(where(is.numeric), as.integer))
```

```{r cv}
fitControl <- trainControl(method = "cv",
                           number = 100) 
                           
reg_cv <- train(count_complaints ~ ., 
                data = reg_data %>% select(-mapname) %>% st_drop_geometry(), 
                 method = "glm", 
                 trControl = fitControl)  

reg_cv_sum <- data.frame(preds = reg_cv$finalModel$fitted.values)
reg_cv_sum$truth <- reg_cv$trainingData$`.outcome`
reg_cv_sum <- reg_cv_sum %>%
                  mutate(abs_error = abs(preds - truth))

to_map <- cbind(full_net, reg_cv_sum)

tm <- tm_shape(to_map) +
  tm_polygons(col = "preds", border.alpha = 0, style = "fisher", palette = "viridis")

tmap_theme(tm, "Predicted Illegal Dumping Hotspots\n100-Fold Cross Validation")
```

```{r spcv}
cluster_folds <- spatial_leave_location_out_cv(st_drop_geometry(reg_data), mapname)
model_spec <- poisson_reg(mode = "regression") %>% set_engine("glm")
x <- recipes::recipe(count_complaints ~ count_vacant + cars_count + outage_count + permits_count, data = st_drop_geometry(reg_data))

results <- fit_resamples(model_spec, x, resamples = cluster_folds, metrics = metric_set(mae), control = control_grid(save_pred = TRUE))
predictions <- collect_predictions(results)
metrics <- collect_metrics(results, summarize = FALSE)
overall_metrics <- collect_metrics(results)
  
preds_net <- left_join(phl_grid, predictions, by = c("grid_id" = ".row")) %>%
                mutate(abs_error = abs(.pred - count_complaints)) %>%
                st_as_sf()

tm <- tm_shape(preds_net) +
  tm_polygons(col = ".pred", border.alpha = 0, style = "fisher", palette = "viridis")
tmap_theme(tm, "Predicted Illegal Dumping Hotspots\nSpatial Cross Validation")

tm <- tm_shape(preds_net) +
  tm_polygons(col = "abs_error", border.alpha = 0, style = "fisher", palette = "viridis")
tmap_theme(tm, "Predicted Illegal Dumping Prediction Errors\nSpatial Cross Validation")
```

Interestingly, we note that, while the regular k-fold cross validation produces normally-distributed errors, the spatial k-fold cross validation produces a bimodal (or possibly trimodal) distribution, suggesting that perhaps different neighborhoods are prone to much higher rates of prediction error.
```{r prediction hist}

reg_cv_mae_results <- data.frame(MAE = reg_cv$resample$MAE)

cv_mae_hist <- ggplot(reg_cv_mae_results, aes(x = MAE)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of MAE per Iteration",
       subtitle = "Regular 100-Fold Cross-Validation",
       x = "MAE",
       y = "Count")

spcv_mae_hist <- ggplot(metrics, aes(x = .estimate)) +
  geom_histogram(bins = 10) +
  labs(title = "Distribution of MAE per Iteration",
       subtitle = "Spatial K-Fold Cross-Validation",
       x = "MAE",
       y = "Count")

ggarrange(cv_mae_hist, spcv_mae_hist)
```

We note taht the MAE with spatial cross-validation is much higher than the MAE for regular 100-fold cross-validation (twice as high, in fact). 
```{r mae x model}
mae_df <- data.frame(
  model = c("100-Fold CV", "Spatial CV"),
  mae = c(reg_cv$results$MAE, overall_metrics$mean)
)

library(kableExtra)

mae_df %>%
  mutate(mae = round(mae, 2)) %>%
  kbl() %>%
  kable_minimal(full_width = F)
```

## Discussion

We noted that cell size had an impact on the accuracy of the model as well; larger cell sizes corresponded to 1) higher Pearson's correlation coefficients with predictors, and 2) more consistent MAEs when running spatial cross-validation. Based on this, we settled on a cell size of 2,500" after having tried both 1,000" and 500" previously. We take this to mean that larger cell sizes give up some localized precision while making the model on the whole more accurate (i.e., although they are predicting at broader geographic scales, the range of errors is smaller than with smaller cell sizes).

### Accuracy

### Generalizability

## Conclusion