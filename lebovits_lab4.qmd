---
title: "Targeting Housing Subsidies"
subtitle: "MUSA 508, Lab #4"
author: "Nissim Lebovits"
date: today
project:
  type: website
  output-dir: docs
format: 
  html:
    embed-resources: true
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
    number-sections: true
editor: source
execute:
  warning: false
  error: false
  messages: false
  echo: true
  cache: false
---

Some things to check for:

- balanced data in train/test split (need to figure this out)
- compare it to a random forest classifier
- scale data
- try glmnet (penalized/lasso regression)

So we want:

- one kitchen sink model
- one model that is scaled, log transformed where applicable, has correlation removed, and removes non-significant predictors


How to account for class imbalance in our dataset?

Based on crosstab, drop the following categorical variables:

- poutcome
- day of the week
- taxbill_in_phl
- mortgage

Create binary for university degree

Also drop unemployment rate and inflation rate due to high correlation with other vars

## Summary
```{r setup, set.seed(40)}
library(tidyverse)
library(caret)
library(ggthemr)
library(pscl)
library(plotROC)
library(pROC)
library(scales)
library(rstatix)
library(ggpubr)
library(kableExtra)
library(randomForest)
library(crosstable)
library(ggcorrplot)
library(crosstable)
library(rsample)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

options(scipen = 999)

ggthemr('pale')

housing_subsidy_path <- "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter6/housingSubsidy.csv"
housing_subsidy_data <- read.csv(housing_subsidy_path)
# log transform previous, pdays, and campagin


housing_subsidy_data <- housing_subsidy_data %>%
                          mutate(education = case_when(
                            education == "basic.4y" ~ "basic.4y or less",
                            education == "illiterate" ~ "basic.4y or less",
                            TRUE ~ education
                          ))

housing_subsidy_data <- housing_subsidy_data %>%
                          mutate(taxLien = case_when(
                            taxLien == "yes" ~ "unknown/yes",
                            taxLien == "unknown" ~ "unknown/yes",
                            TRUE ~ taxLien
                          ))


drop_vars <- c("mortgage", 
               "taxbill_in_phl", 
               "day_of_week", 
               "education", 
              # "inflation_rate",
               "unemploy_rate", 
              # "previous", 
               "X"# , 
              # "age"
               )  

hs_data_eng <- housing_subsidy_data %>%
  mutate(
    #campaign = log(campaign),
    cci_peak_50_indicator = ifelse(cons.conf.idx >= -51 & cons.conf.idx <= -49, 1, 0),
    cci_peak_40_indicator = ifelse(cons.conf.idx >= -41 & cons.conf.idx <= -39, 1, 0),
    cci_peak_35_indicator = ifelse(cons.conf.idx >= -36 & cons.conf.idx <= -34, 1, 0),
    cpi_peak_92_5_indicator = ifelse(cons.price.idx >= 92.3 & cons.price.idx <= 92.7, 1, 0),
    cpi_peak_93_5_indicator = ifelse(cons.price.idx >= 93.3 & cons.price.idx <= 93.7, 1, 0),
    cpi_peak_94_5_indicator = ifelse(cons.price.idx >= 94.3 & cons.price.idx <= 94.7, 1, 0),
    ir_peak_1_5_indicator = ifelse(inflation_rate >= 1.3 & inflation_rate <= 1.7, 1, 0),
    ir_peak_4_5_indicator = ifelse(inflation_rate >= 4.3 & inflation_rate <= 4.7, 1, 0),
    sor_peak_5025_indicator = ifelse(spent_on_repairs >= 5020 & spent_on_repairs <= 5030, 1, 0),
    sor_peak_5075_indicator = ifelse(spent_on_repairs >= 5070 & spent_on_repairs <= 5080, 1, 0),
    sor_peak_5110_indicator = ifelse(spent_on_repairs >= 5105 & spent_on_repairs <= 5115, 1, 0),
    sor_peak_5175_indicator = ifelse(spent_on_repairs >= 5170 & spent_on_repairs <= 5180, 1, 0),
    ir_peak_1_5_indicator = ifelse(inflation_rate >= 1.3 & inflation_rate <= 1.7, 1, 0),
    ir_peak_4_5_indicator = ifelse(inflation_rate >= 4.3 & inflation_rate <= 4.7, 1, 0),
    ur_peak_4_5_indicator = ifelse(unemploy_rate >= 4.3 & unemploy_rate <= 4.7, 1, 0),
    univ_deg = ifelse(education == "university.degree", 1, 0),
    pdays_special = ifelse(pdays >= 999, 1, 0),
    has_previous_contact = ifelse(previous > 0, 1, 0)
    ) %>%
  select(-c(drop_vars))


```


## Motivation
Emil City is considering a more proactive approach for targeting home owners who qualify for a home repair tax credit program. This tax credit program has been around for close to twenty years, and while the Department of Housing and Community Development (HCD) tries to proactively reach out to eligible homeowners ever year, the uptake of the credit is woefully inadequate. Typically not all eligible homeowners they reach out to who enter the program ultimately take the credit.

The consensus at HCD is that the low conversion rate is due to the fact that the agency reaches out to eligible homeowners at random. To move toward a more targeted campaign, we attempt to use a binomial logistic regression to assess whether client-level data collected from previous campaigns can drive decision-making to better target limited outreach sources and approve the success rates in getting homeowners into the home repair tax credit program.


## Methods
### Data
For this analysis, we use historic data collected by Emil City. To start, we assess the relationship of the various potential predictors to our dependent variable, `y`, and the relationship of the predictors to each other. We do this using a correlation plot, t-tests (for continuous variables), and chi-squared tests (for categorical variables). We are looking for variables that 1) have a statistically significant relationship with our dependent variable, `y`, and 2) do not exhibit strong multicolinearity with each other. We will first run a logistic regression on our full dataset and then, using these analytic tools, attempt to eliminate unhelpful features and/or engineer more helpful ones to create the most parsimonous model which maximizes the cost-benefits tradeoffs of the outreach approach.

#### Multicolinearity
Using a correlation plot, we can assess which predictors are multicolinear and therefore should be removed from our model. For our purposes, we define strong multicolinearity as anything with an r-value of greater than 0.9 or less than -0.9, which signals a redundancy in including both predictors (in other words, adding the second one does not help improve the predictive power of the model). In the case of multicollinearity, we usually leave one predictor and take out the rest that are strongly correlated with that one predictor. Here, we only drop two predictors: inflation rate and unemployment rate.
```{r corrplot}

corr <- round(cor(housing_subsidy_data %>% select(where(is.numeric))), 1)
p.mat <- cor_pmat(housing_subsidy_data %>% select(where(is.numeric)))

ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE,
    type = "lower", insig = "blank", lab = TRUE)
```
#### T-Tests for Continuous Variables
Using t-tests, we can assess whether continuous predictors differ across the two classes of the dependent variable in a statistically significant way. If they do, they are useful predictors for our model, but if they do not, they do not contribute meaningfully to the model. Based on these t-tests, we find that one of our continuous variables, `X`, is shows no statistically significant difference across classes and therefore can be discarded.
```{r ttests}
#| tbl-cap: T-Tests for Continuous Variables

ttest <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric") %>%
  group_by(variable) %>%
  rstatix::t_test(value ~ y) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance() %>%
  select(variable,
         p,
         p.adj,
         p.adj.signif)

ttest %>%
  kbl() %>%
  kable_minimal()
```

#### Chi-Squared Tests for Categorical Predictors
Similarly, we can use chi-squared tests to assess whether categorical predictors show statistically signficant differences across the classes of the dependent variable. Based on these tests, we drop four predictors: "mortgage", "taxbill_in_phl", "day_of_week", and "education".
```{r crosstabs}
#| tbl-cap: Cross Tabulation of Categorical Variables

cat_vars <- colnames(housing_subsidy_data %>% select(where(is.character)))

crosstable(housing_subsidy_data, 
           cat_vars, 
           by=y, 
           total="both", 
           percent_pattern="{n} ({p_row}/{p_col})", 
           percent_digits=0, 
           test = TRUE) %>%
  as_flextable()
```

Split your data into a 65/35 training/test set.
The Sensitivity (True Positive Rate) for a model with all the features is very low. Engineer new features that significantly increase the Sensitivity.

Interpret your new features in one paragraph.
```{r numeric feature viz}
hmm1 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric")

ggplot(hmm1) +
  geom_density(aes(x = value, fill = y)) +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Density", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Continous outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))


hmm <- hmm1 %>%
  group_by(variable, y) %>%
  summarise(mean_value = mean(value, na.rm = TRUE))

ggplot(hmm) +
  geom_col(aes(y = mean_value, x = y, fill = y)) +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Mean Value", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Continous outcomes)")
```


```{r categorical feature viz}
hmm2 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.character), names_to = "variable", values_to = "value") %>%
  filter(!variable == "y") %>%
  select(y_numeric, value, variable) %>%
  group_by(y_numeric, variable, value) %>%
  summarize(count = n())

hmm_counts <- hmm2 %>%
                group_by(y_numeric, variable) %>%
                summarize(total = sum(count))

hmm2 <- left_join(hmm2, hmm_counts) %>%
            mutate(pct = count/total*100)

ggplot(hmm2) +
  geom_col(aes(x = value, y = pct, fill = as.factor(y_numeric)), position = "dodge") +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Mean Value", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Categorical outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))
```

We find that the singular occurrence of "illiterate" in our base dataset throws off our model, so we group it in with the "basic.4y" education category as "basic.4y or less".


Likewise, we find that the singular occurrence of "yes" in our base dataset throws off our model, so we group it in with the "unknown" tax lien category as "unknown/yes".

### Model
```{r model and eval fn}

set.seed(42)

model_and_eval <- function(data) {
  
trainIndex <- initial_split(data, prop = 0.65, strata = y)
houseSubTrain <- training(trainIndex)
houseSubTest  <-  testing(trainIndex)

data_for_model <- houseSubTrain %>% dplyr::select(-y)

hs_reg <- glm(y_numeric ~ ., data = data_for_model, family = binomial(link = "logit"))

print(summary(hs_reg))

print(pR2(hs_reg)[4]) # mcfadden's R-squared--0.2 to 0.4 are considered good model

testProbs <- data.frame(outcome = as.factor(houseSubTest$y_numeric),
                        probs = predict(hs_reg, houseSubTest, type = "response"))%>%
                        mutate(pred_outcome  = as.factor(ifelse(probs > 0.5 , 1, 0)))

print(ggplot(testProbs, aes(x = probs, fill = as.factor(outcome))) +
  geom_density(alpha = 0.7) +
  facet_grid(outcome ~ .) +
  xlim(0, 1) +
  labs(x = "Response", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome") +
  plotTheme() + theme(strip.text.x = element_text(size = 18),
        legend.position = "none"))

print(caret::confusionMatrix(testProbs$pred_outcome, testProbs$outcome,
                       positive = "1"))

ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit <- train(y ~ .,
                  data= houseSubTrain %>% dplyr::select(
                                                    -y_numeric
                                                    ),
                method="glm",
                family="binomial",
                metric="ROC",
                trControl = ctrl)

cvFit


print(
  dplyr::select(cvFit$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) +
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines") +
    plotTheme()
)

auc <- round(pROC::auc(testProbs$outcome, testProbs$probs), 3)
roc_subtitle = paste0("AUC: ", auc)

print(

ggplot(testProbs, aes(d = as.numeric(outcome), m = probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve",
       subtitle = roc_subtitle )
)

cost_benefit_table <-
   testProbs %>%
      count(pred_outcome, outcome) %>%
      summarize(True_Negative = sum(n[pred_outcome==0 & outcome==0]),
                True_Positive = sum(n[pred_outcome==1 & outcome==1]),
                False_Negative = sum(n[pred_outcome==0 & outcome==1]),
                False_Positive = sum(n[pred_outcome==1 & outcome==0])) %>%
       gather(Variable, Count) %>%
       mutate(Revenue =
               case_when(Variable == "True_Negative"  ~ 0,
                         Variable == "True_Positive"  ~ 12400 * Count,
                         Variable == "False_Negative" ~ 0,
                         Variable == "False_Positive" ~  Count * -2850)) %>%
    bind_cols(data.frame(Description = c(
              "Predicted correctly homeowner would not enter the credit program, no marketing resources were allocated, and no credit was allocated.",
              "Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit",
              "We predicted that a homeowner would not enter the credit program but they did.",
              "Predicted incorrectly that a homeowner would enter the credit program.")))

print(cost_benefit_table)

whichThreshold <-
  iterateThresholds(
     data=testProbs, observedClass = outcome, predictedProbs = probs)

whichThreshold[1:5,]

whichThreshold <-
  whichThreshold %>%
    dplyr::select(starts_with("Count"), Threshold) %>%
    gather(Variable, Count, -Threshold) %>%
    mutate(Revenue =
             case_when(Variable == "Count_TN"  ~ 0,
                       Variable == "Count_TP"  ~ 12400 * Count,
                       Variable == "Count_FN"  ~ 0,
                       Variable == "Count_FP"  ~ Count * -2850))

print(
whichThreshold %>%
  ggplot(.,aes(Threshold, Revenue, colour = Variable)) +
  geom_point() +
  # scale_colour_manual(values = palette5[c(5, 1:3)]) +
  labs(title = "Revenue by confusion matrix type and threshold",
       y = "Revenue") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix"))
)

whichThreshold_revenue <-
  whichThreshold %>%
    mutate(actualChurn = ifelse(Variable == "Count_TP", (Count * .5),
                         ifelse(Variable == "Count_FN", Count, 0))) %>%
    group_by(Threshold) %>%
    summarize(Revenue = sum(Revenue),
              Actual_Churn_Rate = sum(actualChurn) / sum(Count),
              Actual_Churn_Revenue_Loss =  sum(actualChurn * 30),
              Revenue_Next_Period = Revenue - Actual_Churn_Revenue_Loss)

print(whichThreshold_revenue[1:5,])

print(
whichThreshold_revenue %>%
  dplyr::select(Threshold, Revenue, Revenue_Next_Period) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_vline(xintercept = pull(arrange(whichThreshold_revenue, -Revenue)[1,1])) +
   # scale_colour_manual(values = palette2) +
    plotTheme() + ylim(0,70000) +
    labs(title = "Revenue this pay period and the next by threshold",
         subtitle = "Assuming no new customers added next period. Vertical line denotes optimal threshold")
)

}
```

```{r model 1 setup}
model_and_eval(housing_subsidy_data)
```

```{r model 2 setup}
model_and_eval(hs_data_eng)
```

## Results
Show a regression summary for both the kitchen sink and your engineered regression.

### Prediction Accurcay
Cross validate both models; compare and interpret two facetted plots of ROC, Sensitivity and Specificity.


Output an ROC curve for your new model and interpret it.


### Cost-Benefit Analysis

Because the benefits of a true positive significantly outweight the cost of a false positive, we want to optimize our model to optimize sensitivity (true positive rate).

Develop a cost benefit analysis.

(This needs to be updated.)


Write out the cost/benefit equation for each confusion metric.

Create the ‘Cost/Benefit Table’ as seen above.

Plot the confusion metric outcomes for each Threshold.

Create two small multiple plots that show Threshold as a function of Total_Revenue and Total_Count_of_Credits. Interpret this.
Create a table of the Total_Revenue and Total_Count_of_Credits allocated for 2 categories. 50%_Threshold and your Optimal_Threshold.



## Discussion

## Conclusion
Conclude whether and why this model should or shouldn’t be put into production. What could make the model better? What would you do to ensure that the marketing materials resulted in a better response rate?


