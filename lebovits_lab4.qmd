---
title: "Targeting Housing Subsidies"
subtitle: "MUSA 508, Lab #4"
author: "Nissim Lebovits"
date: today
project:
  type: website
  output-dir: docs
format: 
  html:
    embed-resources: true
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
    number-sections: true
editor: source
execute:
  warning: false
  error: false
  messages: false
  echo: true
  cache: false
---

## Summary

Emil City is considering a more proactive approach for targeting home owners who qualify for a home repair tax credit program. Based on a dataset from previous years, we implement a logistic regression to predict program enrollment and assess whether the use of a predictive model can improve the net revenue produced by the program. We compare the base dataset to a dataset including several engineered features. We find that, in both cases, the net revenue is a significant improvement over the current process of random outreach. We further find that feature engineering can improve the performance of the model and inform future outreach. Although we note several ways in which the model could be improved, we recommend that it be put into production given that even a moderately successful preditive model offers a substantial improvement over random outreach.

```{r setup, set.seed(40)}
library(tidyverse)
library(caret)
library(ggthemr)
library(pscl)
library(plotROC)
library(pROC)
library(scales)
library(rstatix)
library(ggpubr)
library(kableExtra)
library(crosstable)
library(ggcorrplot)
library(crosstable)
library(rsample)
library(gridExtra)
library(scales)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
source("hw4_fns.R")

options(scipen = 999)

ggthemr('pale')

housing_subsidy_path <- "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter6/housingSubsidy.csv"
housing_subsidy_data <- read.csv(housing_subsidy_path)
# log transform previous, pdays, and campagin

# collapse classes to avoid resampling issues in train/test split (only one observation of "illiterate" and "yes")
housing_subsidy_data <- housing_subsidy_data %>%
                          mutate(education = case_when(
                            education == "basic.4y" ~ "basic.4y or less",
                            education == "illiterate" ~ "basic.4y or less",
                            TRUE ~ education
                          ),taxLien = case_when(
                            taxLien == "yes" ~ "unknown/yes",
                            taxLien == "unknown" ~ "unknown/yes",
                            TRUE ~ taxLien
                          ))
```

## Motivation

Emil City is considering a more proactive approach for targeting home owners who qualify for a home repair tax credit program. This tax credit program has been around for close to twenty years, and while the Department of Housing and Community Development (HCD) tries to proactively reach out to eligible homeowners ever year, the uptake of the credit is woefully inadequate. Typically not all eligible homeowners they reach out to who enter the program ultimately take the credit.

The consensus at HCD is that the low conversion rate is due to the fact that the agency reaches out to eligible homeowners at random. To move toward a more targeted campaign, we attempt to use a binomial logistic regression to assess whether client-level data collected from previous campaigns can drive decision-making to better target limited outreach sources and approve the success rates in getting homeowners into the home repair tax credit program.

## Methods

### Data

For this analysis, we use historic data collected by Emil City. To start, we assess the relationship of the various potential predictors to our dependent variable, `y`, and the relationship of the predictors to each other. We do this using a correlation plot, t-tests (for continuous variables), and chi-squared tests (for categorical variables). We are looking for variables that 1) have a statistically significant relationship with our dependent variable, `y`, and 2) do not exhibit strong multicolinearity with each other. We will first run a logistic regression on our full dataset and then, using these analytic tools, attempt to eliminate unhelpful features and/or engineer more helpful ones to create the most parsimonous model which maximizes the cost-benefits tradeoffs of the outreach approach. Some minor data cleaning is required: we find that the singular occurrence of "illiterate" in the `education` column in our base dataset makes it impossible to resample properly for our model, so we group it in with the "basic.4y" category as "basic.4y or less". Likewise, we group the singular occurence of "yes" in the `taxLien` column in with the "unknown" category as "unknown/yes".

#### Multicolinearity

Using a correlation plot, we can assess which predictors are multicolinear and therefore should be removed from our model. For our purposes, we define strong multicolinearity as anything with an r-value of greater than 0.9 or less than -0.9, which signals a redundancy in including both predictors (in other words, adding the second one does not help improve the predictive power of the model). In the case of multicollinearity, we usually leave one predictor and take out the rest that are strongly correlated with that one predictor. Here, we only drop two predictors: inflation rate and unemployment rate.

```{r corrplot}

corr <- round(cor(housing_subsidy_data %>% select(where(is.numeric))), 1)
p.mat <- cor_pmat(housing_subsidy_data %>% select(where(is.numeric)))

ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE,
    type = "full", insig = "blank", lab = TRUE)
```

#### T-Tests for Continuous Variables

Using t-tests, we can assess whether continuous predictors differ across the two classes of the dependent variable in a statistically significant way. If they do, they are useful predictors for our model, but if they do not, they do not contribute meaningfully to the model. Based on these t-tests, we find that one of our continuous variables, `X`, is shows no statistically significant difference across classes and therefore can be discarded.

```{r ttests}
#| tbl-cap: T-Tests for Continuous Variables

ttest <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric") %>%
  group_by(variable) %>%
  rstatix::t_test(value ~ y) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance() %>%
  select(variable,
         p,
         p.adj,
         p.adj.signif)

ttest %>%
  kbl() %>%
  kable_minimal()
```

#### Chi-Squared Tests for Categorical Predictors

Similarly, we can use chi-squared tests to assess whether categorical predictors show statistically signficant differences across the classes of the dependent variable. Based on these tests, we drop four predictors: "mortgage", "taxbill_in_phl", "day_of_week", and "education".

```{r crosstabs}
#| tbl-cap: Cross Tabulation of Categorical Variables

cat_vars <- colnames(housing_subsidy_data %>% select(where(is.character)))

crosstable(housing_subsidy_data, 
           cat_vars, 
           by=y, 
           total="both", 
           percent_pattern="{n} ({p_row}/{p_col})", 
           percent_digits=0, 
           test = TRUE) %>%
  as_flextable()
```

#### Feature Engineering
We further attempt to exploit differences in the distributions of the continuous and categorical variables in our dataset by engineering new features that highlight the nuanced ways in which they diverge.
```{r numeric feature viz}
hmm1 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric")

ggplot(hmm1) +
  geom_density(aes(x = value, fill = y)) +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Density", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Continous outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))


hmm2 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.character), names_to = "variable", values_to = "value") %>%
  filter(!variable == "y") %>%
  select(y_numeric, value, variable) %>%
  group_by(y_numeric, variable, value) %>%
  summarize(count = n())

hmm_counts <- hmm2 %>%
                group_by(y_numeric, variable) %>%
                summarize(total = sum(count))

hmm2 <- left_join(hmm2, hmm_counts) %>%
            mutate(pct = count/total*100)

ggplot(hmm2) +
  geom_col(aes(x = value, y = pct, fill = as.factor(y_numeric)), position = "dodge") +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Mean Value", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Categorical outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))

```

Based on these distributions, we implement some of the following approaches (this is not a comprehensive list; full engineering can be seen in the code below):

 - Use binary markers to indicate whether the consumer confidence index (CCI) for a given observation falls into one of the observed peaks in CCI associated with higher rates of credit enrollment
 - Create a binary variable to indicate the presence of a university degree
 - Create a binary category for months to indicate whether a given observation occurred in a month associated with a higher rate of credit enrollment
 
We also drop redundant variables following these transformations. 

```{r engineer}
drop_vars <- c("mortgage", 
               "taxbill_in_phl", 
               "day_of_week", 
               "education", 
                "inflation_rate",
               "unemploy_rate", 
              "cons.conf.idx",
              "cons.price.idx",
              "spent_on_repairs",
              "unemploy_rate",
              "pdays",
              "previous",
              "job",
               "X" , 
              "age",
              "month"
               )  

hs_data_eng <- housing_subsidy_data %>%
  mutate(
    cci_peak_50_indicator = ifelse(cons.conf.idx >= -51 & cons.conf.idx <= -49, 1, 0),
    cci_peak_40_indicator = ifelse(cons.conf.idx >= -41 & cons.conf.idx <= -39, 1, 0),
    cci_peak_35_indicator = ifelse(cons.conf.idx >= -36 & cons.conf.idx <= -34, 1, 0),
    cpi_peak_92_5_indicator = ifelse(cons.price.idx >= 92.3 & cons.price.idx <= 92.7, 1, 0),
    cpi_peak_93_5_indicator = ifelse(cons.price.idx >= 93.3 & cons.price.idx <= 93.7, 1, 0),
    cpi_peak_94_5_indicator = ifelse(cons.price.idx >= 94.3 & cons.price.idx <= 94.7, 1, 0),
    sor_peak_5025_indicator = ifelse(spent_on_repairs >= 5020 & spent_on_repairs <= 5030, 1, 0),
    sor_peak_5075_indicator = ifelse(spent_on_repairs >= 5070 & spent_on_repairs <= 5080, 1, 0),
    sor_peak_5110_indicator = ifelse(spent_on_repairs >= 5105 & spent_on_repairs <= 5115, 1, 0),
    sor_peak_5175_indicator = ifelse(spent_on_repairs >= 5170 & spent_on_repairs <= 5180, 1, 0),
    ir_peak_1_5_indicator = ifelse(inflation_rate >= 1.3 & inflation_rate <= 1.7, 1, 0),
    ir_peak_4_5_indicator = ifelse(inflation_rate >= 4.3 & inflation_rate <= 4.7, 1, 0),
    ur_peak_4_5_indicator = ifelse(unemploy_rate >= 4.3 & unemploy_rate <= 4.7, 1, 0),
    univ_deg = ifelse(education == "university.degree", 1, 0),
    pdays_special = ifelse(pdays >= 999, 1, 0),
    has_previous_contact = ifelse(previous > 0, 1, 0),
    special_months = ifelse(month %in% c("dec", "mar", "oct", "sep"), 1, 0),
    job_grouped = case_when(
      job %in% c("blue-collar", "services") ~ "blue-collar_services",
      job %in% c("admin.", "management") ~ "admin_management",
      TRUE ~ as.character(job)
    )
    ) %>%
  select(-c(drop_vars))
```

### Model
Below, we use a binomial logistic regression to predict the classifications of our dependent variable, `y`. A logistic regression estimates, based on predictor variables, the probability that the dependent variable falls within one of the two classes. By setting a probability threshold (typically > 0.5), we can classify the observations. In logistic regression, our primary outcomes of interest are specificity, sensitivity, and the misclassification rate. Sensitivity (also called the true positive rate) measures the proportion of actual positives which are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition), and is complementary to the false negative rate. Specificity (also called the true negative rate) measures the proportion of negatives which are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition), and is complementary to the false positive rate. The misclassification rate is the proportion of the sum of false positives and false negatives out of the total number of observations in the sample. Although the standard approach is to pursue balanced classification accuracy--that is, a high true positive rate and a low false positive rate--in this particular instance, we seek to maximize the true positive rate above all else, given that the returns on a true positive outweigh the loss on a false positive by a factor of roughly five.

## Results
### Regression Results
#### Base Model
```{r model 1 setup}
set.seed(153)

trainIndex_base <- initial_split(housing_subsidy_data, prop = 0.65, strata = y)
houseSubTrain_base <- training(trainIndex_base)
houseSubTest_base  <-  testing(trainIndex_base)

data_for_model_base <- houseSubTrain_base %>% dplyr::select(-y)

hs_reg_base <- glm(y_numeric ~ ., data = data_for_model_base, family = binomial(link = "logit"))

print(summary(hs_reg_base))

print(pR2(hs_reg_base)[4]) # mcfadden's R-squared--0.2 to 0.4 are considered good model

testProbs_base <- data.frame(outcome = as.factor(houseSubTest_base$y_numeric),
                        probs = predict(hs_reg_base, houseSubTest_base, type = "response"))%>%
                        mutate(pred_outcome  = as.factor(ifelse(probs > 0.5 , 1, 0)))


print(caret::confusionMatrix(testProbs_base$pred_outcome, testProbs_base$outcome,
                       positive = "1"))


ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit_base <- train(y ~ .,
               data= houseSubTrain_base %>% dplyr::select(-y_numeric),
                method="glm",
                family="binomial",
                metric="ROC",
                trControl = ctrl)

cvFit_base
```

#### Feature Engineered Model
```{r model 2 setup}
set.seed(1353)

trainIndex_eng <- initial_split(hs_data_eng, prop = 0.65, strata = y)
houseSubTrain_eng <- training(trainIndex_eng)
houseSubTest_eng  <- testing(trainIndex_eng)

data_for_model_eng <- houseSubTrain_eng %>% dplyr::select(-y)

hs_reg_eng <- glm(y_numeric ~ ., data = data_for_model_eng, family = binomial(link = "logit"))

print(summary(hs_reg_eng))

print(pR2(hs_reg_eng)[4]) # mcfadden's R-squared--0.2 to 0.4 are considered good models

testProbs_eng <- data.frame(outcome = as.factor(houseSubTest_eng$y_numeric),
                        probs = predict(hs_reg_eng, houseSubTest_eng, type = "response")) %>%
                        mutate(pred_outcome  = as.factor(ifelse(probs > 0.5 , 1, 0)))


print(caret::confusionMatrix(testProbs_eng$pred_outcome, testProbs_eng$outcome,
                       positive = "1"))


ctrl <- trainControl(method = "cv", number = 100, classProbs = TRUE, summaryFunction = twoClassSummary)

cvFit_eng <- train(y ~ .,
               data= houseSubTrain_eng %>% dplyr::select(-y_numeric),
                method="glm",
                family="binomial",
                metric="ROC",
                trControl = ctrl)

cvFit_eng
```

### Prediction Accuracy
We find that our feature engineering does marginally increase the balanced accuracy of the model, as indicated by the increase in AUC. It is, comparatively speaking, a better model than the kitchen sink approach, although in absolute terms, it is only a moderately successful model. However, as discussed below, its increased sensitivity has meaningful implications for its functional utility in revenue generation and service delivery in the public sector.

```{r prob density}

prob_dens_base <- plot_prob_dens(testProbs_base, "Base Data")

prob_dens_eng <- plot_prob_dens(testProbs_eng, "Engineered Data")

ggarrange(prob_dens_base, prob_dens_base, nrow = 2)

```

```{r plot gof}
gof_base <- plot_gof(cvFit_base, "Base Data")

gof_eng <- plot_gof(cvFit_eng, "Engineered Data")

ggarrange(gof_base, gof_eng, nrow = 2)
```

```{r plot roc}
roc_base <- roc_plot(testProbs_base, "Base Data")

roc_eng <- roc_plot(testProbs_eng, "Engineered Data")

ggarrange(roc_base, roc_eng, ncol = 2)
```
### Cost-Benefit Analysis
In keeping with the goals of this project, we are not seeking to achieve a balanced accuracy, but rather to maximize revenue according to our cost benefit analysis. Per our assumptions, the credit allocates \$5,000 per homeowner which can be used toward home improvement. Academic researchers in Philadelphia evaluated the program finding that houses that transacted after taking the credit, sold with a \$10,000 premium, on average. Homes surrounding the repaired home see an aggregate premium of \$56,000, on average.

-   True Positive - Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit. Per the formula below, we count a net revenue gain of $12,400 per true positive.

    $(0.25(66,000 - 5,000 - 2,850) - 0.75 (2,850)) * count = 12,400 * count$

-   True Negative - Predicted correctly homeowner would not enter the credit program, no marketing resources were allocated, and no credit was allocated. Thus, the net revenue gain for a true negative is \$0.

-   False Positive - Predicted incorrectly homeowner would enter the credit program; allocated marketing resources; no credit allocated. We count a net revenue loss of \$2,850 per false positive.

-   False Negative - We predicted that a homeowner would not enter the credit program but they did. These are likely homeowners who signed up for reasons unrelated to the marketing campaign. Thus, the net revenue gain for a false negative is \$0.

Because the benefits of a true positive significantly outweigh the cost of a false positive (we earn \$12,400 per true positive, while only losing \$2,850 per false positive), we want to optimize our model to optimize sensitivity (true positive rate), rather than balanced accuracy.

```{r cb tabs base}
print_cb_tab(testProbs_base, "Cost-Benefit Analysis with Base Data")
```

```{r cb tabs eng}
print_cb_tab(testProbs_eng, "Cost-Benefit Analysis with Engineered Data")
```

```{r conf matrix}
thresh_base <- make_thresh(testProbs_base)
thresh_eng <- make_thresh(testProbs_eng)

thresh_tab_base <- print_thresh_tab(thresh_base, "Base Data")
thresh_tab_eng <- print_thresh_tab(thresh_eng, "Engineered Data")

ggarrange(thresh_tab_base, thresh_tab_eng, nrow = 2)
```

Based on our test datasets, we find that our engineered dataset does return a slightly higher maximum revenue than our base dataset, at a threshold around 0.15. It is notable that the threshold is so much lower than the standard 0.5. Again, this is because we are not interested in balanced class accuracy in this use case, but rather in maximizing net revenue, which requires prioritizing sensitivity (true positives) above all else.
```{r count + rev plots}

plot_count_base <- plot_count(thresh_base, "Base Data")
plot_rev_base <-plot_rev(thresh_base, "Base Data")

plot_count_eng <- plot_count(thresh_eng, "Engineered Data")
plot_rev_eng <- plot_rev(thresh_eng, "Engineered Data")

plot_count_base
plot_rev_base
plot_count_eng
plot_rev_eng
```

```{r tot rev tab}
thresh_base_credits <- thresh_base %>% 
  mutate(Threshold = as.character(Threshold)) %>%
    group_by(Threshold) %>% 
    filter(Variable == "Count_TP") %>%
    summarize(total_credits = (sum(Count))* 5000 * 0.25) %>%
    mutate(model = "Base Data")

thresh_base_rev <- thresh_base %>% 
  mutate(Threshold = as.character(Threshold)) %>%
    group_by(Threshold) %>% 
    summarize(revenue = sum(Revenue)) %>%
    mutate(model = "Base Data")

thresh_base_dt <- left_join(thresh_base_credits, thresh_base_rev) %>%
    filter(Threshold == 0.5 | revenue == max(revenue))

thresh_eng_credits <- thresh_eng %>% 
  mutate(Threshold = as.character(Threshold)) %>%
    group_by(Threshold) %>% 
    filter(Variable == "Count_TP") %>%
    summarize(total_credits = (sum(Count))* 5000 * 0.25) %>%
    mutate(model = "Engineered Data")


thresh_eng_rev <- thresh_eng %>% 
  mutate(Threshold = as.character(Threshold)) %>%
    group_by(Threshold) %>% 
    summarize(revenue = sum(Revenue)) %>%
    mutate(model = "Engineered Data")

thresh_eng_dt <- left_join(thresh_eng_credits, thresh_eng_rev) %>%
    filter(Threshold == 0.5 | revenue == max(revenue))

thresh_full_dt <- rbind(thresh_base_dt, thresh_eng_dt) %>%
  select(Threshold, model, total_credits, revenue)

thresh_full_dt %>%
    kbl(caption = "Credit Counts and Revenues Across Models and Thresholds") %>%
    kable_minimal()
```

## Discussion

### Importance of Feature Engineering
This exercise speaks to the importance of proper feature engineering in building a model. Several features, such as the consumer price index, were rendered meaningful by examining the full distributions of the data (rather than a summary statistic like the mean) and deriving features from these such as binary markers of peaks in the consumer price index. Further features such as interaction terms could be added in more sophisticated models. The result of this feature engineering was a small but financially meaningful projected net revenue in comparison to our base dataset, indicating that a few hours' worth of data wrangling effort could translate to tens of thousands of dollars in additional revenue.

### Possible New Features
To further improve the model, any number of new features could be added, evaluated, and engineered. Among those that we suspect may improve the model are homeowner race, License & Inspections violations, homeowner tenure, and enrollment in other City programs. As with other features, these should be explored in order to assess correlation and the possibility of deriving additional features from them.

#### Feature Exploration to Guide Outreach
Additionally, we note that a basic understanding of correlations can be used to further guide outreach. Noting that, for example, certain peaks in unemployment rates were correlated with enrollment in the credit, it might be wise to pursue more outreach during comparable future peaks.

### Outcome Prioritization
Another notable component of this exercise is the divergence of academic and public sector priorities in model building. The assumed "best" aim for building a logistic regression model is to maximize balanced class accuracy of the model. In this case, however, given our cost benefit analysis, we found that maximizing sensitivity was most importance, given that the payoff for a true positive was much higher than the cost of a false positive, and that true negatives and false negatives both had net returns of $0. This speaks further to the importance of domain knowledge in data analysis and model building.

### Limitations of Logistic Regression
Finally, this model runs up against the limits of logistic regression. Although logit models have fewer assumptions to satisfy than standard OLS regression, for example, they have weaknesses of their own. For one thing, they typically require very large datasets. A dataset of 40,000 or even 400,000 observations likely would have yielded more accurate results than our dataset of 4,000 observations. This is especially true when you have a severe class imbalance, as we did in our case. Of the `r nrow(housing_subsidy_data)` observations in our dataset, only `r round(nrow(housing_subsidy_data %>% filter(y_numeric == 1))/nrow(housing_subsidy_data) * 100, 1)`% of these observations were instances in which the person in question took the credit. This means that we are likely [dealing with a rare event](https://statisticalhorizons.com/logistic-regression-for-rare-events/), according to which we might find more success with a penalized likelihood approach such as [lasso, ridge, or elastic net regression](https://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/).


## Conclusion
We find that 1) the use of logistic regression as a predictive tool has the potential to increase revenues over the current standard operating practice of random outreach, and 2) that basic feature engineering of the dataset can be used to further increase net revenue using the predictive model. It is true that the bar is low: improving on random outreach, which is no better than a coin flip, is not hard to do. But given how commonly such practices are standard in local governments, this exercise speaks to the large return on investment that can be derived from even basic predictive modeling. The exercise furthermore indicates particular features--such as timing and economic factors--that should be used to guide outreach efforts in order to increase the response rates. Thus, the model (or really, any model with decent sensitivity) is worth putting into production, and the process of building it and improving it can also suggest ways to improve current outreach practices.
