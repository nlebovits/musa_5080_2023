---
title: "Targeting Housing Subsidies"
subtitle: "MUSA 508, Lab #4"
author: "Nissim Lebovits"
date: today
project:
  type: website
  output-dir: docs
format: 
  html:
    embed-resources: true
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
    number-sections: true
editor: source
execute:
  warning: false
  error: false
  messages: false
  echo: true
  cache: false
---

Some things to check for:

-   balanced data in train/test split (need to figure this out)
-   compare it to a random forest classifier
-   scale data
-   try glmnet (penalized/lasso regression)

So we want:

-   one kitchen sink model
-   one model that is scaled, log transformed where applicable, has correlation removed, and removes non-significant predictors

How to account for class imbalance in our dataset?

Based on crosstab, drop the following categorical variables:

-   poutcome
-   day of the week
-   taxbill_in_phl
-   mortgage

Create binary for university degree

Also drop unemployment rate and inflation rate due to high correlation with other vars

## Summary

```{r setup, set.seed(40)}
library(tidyverse)
library(caret)
library(ggthemr)
library(pscl)
library(plotROC)
library(pROC)
library(scales)
library(rstatix)
library(ggpubr)
library(kableExtra)
library(crosstable)
library(ggcorrplot)
library(crosstable)
library(rsample)
library(gridExtra)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

options(scipen = 999)

ggthemr('pale')

housing_subsidy_path <- "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter6/housingSubsidy.csv"
housing_subsidy_data <- read.csv(housing_subsidy_path)
# log transform previous, pdays, and campagin


housing_subsidy_data <- housing_subsidy_data %>%
                          mutate(education = case_when(
                            education == "basic.4y" ~ "basic.4y or less",
                            education == "illiterate" ~ "basic.4y or less",
                            TRUE ~ education
                          ))

housing_subsidy_data <- housing_subsidy_data %>%
                          mutate(taxLien = case_when(
                            taxLien == "yes" ~ "unknown/yes",
                            taxLien == "unknown" ~ "unknown/yes",
                            TRUE ~ taxLien
                          ))


drop_vars <- c("mortgage", 
               "taxbill_in_phl", 
               "day_of_week", 
               "education", 
                "inflation_rate",
               "unemploy_rate", 
              # "previous", 
              "cons.conf.idx",
              "cons.price.idx",
              "spent_on_repairs",
              "unemploy_rate",
              "pdays",
              "previous",
              "job",
               "X" , 
              "age"
               )  

hs_data_eng <- housing_subsidy_data %>%
  mutate(
    cci_peak_50_indicator = ifelse(cons.conf.idx >= -51 & cons.conf.idx <= -49, 1, 0),
    cci_peak_40_indicator = ifelse(cons.conf.idx >= -41 & cons.conf.idx <= -39, 1, 0),
    cci_peak_35_indicator = ifelse(cons.conf.idx >= -36 & cons.conf.idx <= -34, 1, 0),
    cpi_peak_92_5_indicator = ifelse(cons.price.idx >= 92.3 & cons.price.idx <= 92.7, 1, 0),
    cpi_peak_93_5_indicator = ifelse(cons.price.idx >= 93.3 & cons.price.idx <= 93.7, 1, 0),
    cpi_peak_94_5_indicator = ifelse(cons.price.idx >= 94.3 & cons.price.idx <= 94.7, 1, 0),
    sor_peak_5025_indicator = ifelse(spent_on_repairs >= 5020 & spent_on_repairs <= 5030, 1, 0),
    sor_peak_5075_indicator = ifelse(spent_on_repairs >= 5070 & spent_on_repairs <= 5080, 1, 0),
    sor_peak_5110_indicator = ifelse(spent_on_repairs >= 5105 & spent_on_repairs <= 5115, 1, 0),
    sor_peak_5175_indicator = ifelse(spent_on_repairs >= 5170 & spent_on_repairs <= 5180, 1, 0),
    ir_peak_1_5_indicator = ifelse(inflation_rate >= 1.3 & inflation_rate <= 1.7, 1, 0),
    ir_peak_4_5_indicator = ifelse(inflation_rate >= 4.3 & inflation_rate <= 4.7, 1, 0),
    ur_peak_4_5_indicator = ifelse(unemploy_rate >= 4.3 & unemploy_rate <= 4.7, 1, 0),
    univ_deg = ifelse(education == "university.degree", 1, 0),
    pdays_special = ifelse(pdays >= 999, 1, 0),
    has_previous_contact = ifelse(previous > 0, 1, 0),
    special_months = ifelse(month %in% c("dec", "mar", "oct", "sep"), 1, 0),
    job_grouped = case_when(
      job %in% c("blue-collar", "services") ~ "blue-collar_services",
      job %in% c("admin.", "management") ~ "admin_management",
      TRUE ~ as.character(job)
    )
    ) %>%
  select(-c(drop_vars))


```

## Motivation

Emil City is considering a more proactive approach for targeting home owners who qualify for a home repair tax credit program. This tax credit program has been around for close to twenty years, and while the Department of Housing and Community Development (HCD) tries to proactively reach out to eligible homeowners ever year, the uptake of the credit is woefully inadequate. Typically not all eligible homeowners they reach out to who enter the program ultimately take the credit.

The consensus at HCD is that the low conversion rate is due to the fact that the agency reaches out to eligible homeowners at random. To move toward a more targeted campaign, we attempt to use a binomial logistic regression to assess whether client-level data collected from previous campaigns can drive decision-making to better target limited outreach sources and approve the success rates in getting homeowners into the home repair tax credit program.

## Methods

### Data

For this analysis, we use historic data collected by Emil City. To start, we assess the relationship of the various potential predictors to our dependent variable, `y`, and the relationship of the predictors to each other. We do this using a correlation plot, t-tests (for continuous variables), and chi-squared tests (for categorical variables). We are looking for variables that 1) have a statistically significant relationship with our dependent variable, `y`, and 2) do not exhibit strong multicolinearity with each other. We will first run a logistic regression on our full dataset and then, using these analytic tools, attempt to eliminate unhelpful features and/or engineer more helpful ones to create the most parsimonous model which maximizes the cost-benefits tradeoffs of the outreach approach.

#### Multicolinearity

Using a correlation plot, we can assess which predictors are multicolinear and therefore should be removed from our model. For our purposes, we define strong multicolinearity as anything with an r-value of greater than 0.9 or less than -0.9, which signals a redundancy in including both predictors (in other words, adding the second one does not help improve the predictive power of the model). In the case of multicollinearity, we usually leave one predictor and take out the rest that are strongly correlated with that one predictor. Here, we only drop two predictors: inflation rate and unemployment rate.

```{r corrplot}

corr <- round(cor(housing_subsidy_data %>% select(where(is.numeric))), 1)
p.mat <- cor_pmat(housing_subsidy_data %>% select(where(is.numeric)))

ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE,
    type = "full", insig = "blank", lab = TRUE)
```

#### T-Tests for Continuous Variables

Using t-tests, we can assess whether continuous predictors differ across the two classes of the dependent variable in a statistically significant way. If they do, they are useful predictors for our model, but if they do not, they do not contribute meaningfully to the model. Based on these t-tests, we find that one of our continuous variables, `X`, is shows no statistically significant difference across classes and therefore can be discarded.

```{r ttests}
#| tbl-cap: T-Tests for Continuous Variables

ttest <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric") %>%
  group_by(variable) %>%
  rstatix::t_test(value ~ y) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance() %>%
  select(variable,
         p,
         p.adj,
         p.adj.signif)

ttest %>%
  kbl() %>%
  kable_minimal()
```

#### Chi-Squared Tests for Categorical Predictors

Similarly, we can use chi-squared tests to assess whether categorical predictors show statistically signficant differences across the classes of the dependent variable. Based on these tests, we drop four predictors: "mortgage", "taxbill_in_phl", "day_of_week", and "education".

```{r crosstabs}
#| tbl-cap: Cross Tabulation of Categorical Variables

cat_vars <- colnames(housing_subsidy_data %>% select(where(is.character)))

crosstable(housing_subsidy_data, 
           cat_vars, 
           by=y, 
           total="both", 
           percent_pattern="{n} ({p_row}/{p_col})", 
           percent_digits=0, 
           test = TRUE) %>%
  as_flextable()
```

```{r numeric feature viz}
hmm1 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
  filter(variable != "y_numeric")

ggplot(hmm1) +
  geom_density(aes(x = value, fill = y)) +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Density", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Continous outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))


hmm <- hmm1 %>%
  group_by(variable, y) %>%
  summarise(mean_value = mean(value, na.rm = TRUE))

ggplot(hmm) +
  geom_col(aes(y = mean_value, x = y, fill = y)) +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Mean Value", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Continous outcomes)")
```

```{r categorical feature viz}
hmm2 <- housing_subsidy_data %>%
  pivot_longer(cols = where(is.character), names_to = "variable", values_to = "value") %>%
  filter(!variable == "y") %>%
  select(y_numeric, value, variable) %>%
  group_by(y_numeric, variable, value) %>%
  summarize(count = n())

hmm_counts <- hmm2 %>%
                group_by(y_numeric, variable) %>%
                summarize(total = sum(count))

hmm2 <- left_join(hmm2, hmm_counts) %>%
            mutate(pct = count/total*100)

ggplot(hmm2) +
  geom_col(aes(x = value, y = pct, fill = as.factor(y_numeric)), position = "dodge") +
  facet_wrap(~variable, scales = "free") +
    labs(x="Output Var", y="Mean Value", 
         title = "Feature associations with the likelihood of entering a program",
         subtitle = "(Categorical outcomes)") +
  theme(axis.text.x = element_text(hjust = 1, angle = 45))
```

Split your data into a 65/35 training/test set. The Sensitivity (True Positive Rate) for a model with all the features is very low. Engineer new features that significantly increase the Sensitivity.

Interpret your new features in one paragraph.

We find that the singular occurrence of "illiterate" in our base dataset throws off our model, so we group it in with the "basic.4y" education category as "basic.4y or less".

Likewise, we find that the singular occurrence of "yes" in our base dataset throws off our model, so we group it in with the "unknown" tax lien category as "unknown/yes".

### Model

```{r model and eval fn}

set.seed(42)

model_and_eval <- function(data) {
  
trainIndex <- initial_split(data, prop = 0.65, strata = y)
houseSubTrain <- training(trainIndex)
houseSubTest  <-  testing(trainIndex)

data_for_model <- houseSubTrain %>% dplyr::select(-y)

hs_reg <- glm(y_numeric ~ ., data = data_for_model, family = binomial(link = "logit"))

print(summary(hs_reg))

print(pR2(hs_reg)[4]) # mcfadden's R-squared--0.2 to 0.4 are considered good model

testProbs <- data.frame(outcome = as.factor(houseSubTest$y_numeric),
                        probs = predict(hs_reg, houseSubTest, type = "response"))%>%
                        mutate(pred_outcome  = as.factor(ifelse(probs > 0.5 , 1, 0)))

print(ggplot(testProbs, aes(x = probs, fill = as.factor(outcome))) +
  geom_density(alpha = 0.7) +
  facet_grid(outcome ~ .) +
  xlim(0, 1) +
  labs(x = "Response", y = "Density of probabilities",
       title = "Distribution of predicted probabilities by observed outcome") +
  plotTheme() + theme(strip.text.x = element_text(size = 18),
        legend.position = "none"))

print(caret::confusionMatrix(testProbs$pred_outcome, testProbs$outcome,
                       positive = "1"))

ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit <- train(y ~ .,
                  data= houseSubTrain %>% dplyr::select(
                                                    -y_numeric
                                                    ),
                method="glm",
                family="binomial",
                metric="ROC",
                trControl = ctrl)

cvFit


print(
  dplyr::select(cvFit$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) +
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines") +
    plotTheme()
)

auc <- round(pROC::auc(testProbs$outcome, testProbs$probs), 3)
roc_subtitle = paste0("AUC: ", auc)

print(

ggplot(testProbs, aes(d = as.numeric(outcome), m = probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve",
       subtitle = roc_subtitle )
)

cost_benefit_table <-
   testProbs %>%
      count(pred_outcome, outcome) %>%
      summarize(True_Negative = sum(n[pred_outcome==0 & outcome==0]),
                True_Positive = sum(n[pred_outcome==1 & outcome==1]),
                False_Negative = sum(n[pred_outcome==0 & outcome==1]),
                False_Positive = sum(n[pred_outcome==1 & outcome==0])) %>%
       gather(Variable, Count) %>%
       mutate(Revenue =
               case_when(Variable == "True_Negative"  ~ 0,
                         Variable == "True_Positive"  ~ 12400 * Count,
                         Variable == "False_Negative" ~ 0,
                         Variable == "False_Positive" ~  Count * -2850)) %>%
    bind_cols(data.frame(Description = c(
              "Predicted correctly homeowner would not enter the credit program, no marketing resources were allocated, and no credit was allocated.",
              "Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit",
              "We predicted that a homeowner would not enter the credit program but they did.",
              "Predicted incorrectly that a homeowner would enter the credit program.")))

print(cost_benefit_table)

whichThreshold <-
  iterateThresholds(
     data=testProbs, observedClass = outcome, predictedProbs = probs)

print(whichThreshold)

whichThreshold <-
  whichThreshold %>%
    dplyr::select(starts_with("Count"), Threshold) %>%
    gather(Variable, Count, -Threshold) %>%
    mutate(Revenue =
             case_when(Variable == "Count_TN"  ~ 0,
                       Variable == "Count_TP"  ~ 12400 * Count,
                       Variable == "Count_FN"  ~ 0,
                       Variable == "Count_FP"  ~ Count * -2850))

print(
whichThreshold %>%
  ggplot(.,aes(Threshold, Revenue, colour = Variable)) +
  geom_point() +
  # scale_colour_manual(values = palette5[c(5, 1:3)]) +
  labs(title = "Revenue by confusion matrix type and threshold",
       y = "Revenue") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix"))
)

whichThreshold_revenue <-
  whichThreshold %>%
    mutate(actualChurn = ifelse(Variable == "Count_TP", (Count * .5),
                         ifelse(Variable == "Count_FN", Count, 0))) %>%
    group_by(Threshold) %>%
    summarize(Revenue = sum(Revenue),
              Actual_Churn_Rate = sum(actualChurn) / sum(Count),
              Actual_Churn_Revenue_Loss =  sum(actualChurn * 30),
              Revenue_Next_Period = Revenue - Actual_Churn_Revenue_Loss)

print(whichThreshold_revenue)

print(paste0("Maximum Revenue:", max(whichThreshold_revenue$Revenue)))

print(
whichThreshold_revenue %>%
  dplyr::select(Threshold, Revenue, Revenue_Next_Period) %>%
  gather(Variable, Value, -Threshold) %>%
  ggplot(aes(Threshold, Value, colour = Variable)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = pull(arrange(whichThreshold_revenue, -Revenue)[1,1])) + 
    facet_wrap(~ Variable) +
   # scale_colour_manual(values = palette2) +
    plotTheme() + #ylim(0,70000) +
    labs(title = "Revenue this pay period and the next by threshold",
         subtitle = "Assuming no new customers added next period. Vertical line denotes optimal threshold")
)

whichThreshold_count <- 
whichThreshold %>% 
    group_by(Threshold) %>% 
    filter(.,Variable == "True_Positive") %>%
    summarize(total_credits = (sum(Count))* 5000 * 0.25)

countplot <-  ggplot(whichThreshold_count)+
  geom_line(aes(x = Threshold, y = total_credits))+
    labs(title = "Total Credits Applied By Threshold \nFor Test Sample")
  
whichThreshold_revenue <- 
whichThreshold %>% 
    group_by(Threshold) %>% 
    summarize(Revenue = sum(Revenue))

revenueplot <-  ggplot(whichThreshold_revenue)+
  geom_line(aes(x = Threshold, y = Revenue))+
  geom_vline(xintercept =  pull(arrange(whichThreshold_revenue, -Revenue)[1,1]))+
    labs(title = "Model Revenues By Threshold \nFor Test Sample",
         subtitle = "Vertical Line Denotes Optimal Threshold")

print(grid.arrange(countplot, revenueplot, ncol = 2))

}
```

```{r model 1 setup}
model_and_eval(housing_subsidy_data)
```

```{r model 2 setup}
model_and_eval(hs_data_eng)
```

## Results

Show a regression summary for both the kitchen sink and your engineered regression.

### Prediction Accurcay

Cross validate both models; compare and interpret two facetted plots of ROC, Sensitivity and Specificity.

Output an ROC curve for your new model and interpret it.

We find that our feature engineering does marginally increase the balanced accuracy of the model, as indicated by the slight increase in AUC. It is, comparatively speaking, a better model than the kitchen sink approach, although in absolute terms, it is only a moderately successful model.

### Cost-Benefit Analysis

In keeping with the goals of this project, we are not seeking to achieve a balanced accuracy, but rather to maximize revenue according to our cost benefit analysis. Per our assumptions, the credit allocates \$5,000 per homeowner which can be used toward home improvement. Academic researchers in Philadelphia evaluated the program finding that houses that transacted after taking the credit, sold with a \$10,000 premium, on average. Homes surrounding the repaired home see an aggregate premium of \$56,000, on average.

-   True Positive - Predicted correctly homeowner would enter credit program; allocated the marketing resources, and 25% ultimately achieved the credit.

    $(0.25(66,000 - 5,000 - 2,850) - 0.75 (2,850)) * count = 12,400 * count$

-   True Negative - Predicted correctly homeowner would not enter the credit program, no marketing resources were allocated, and no credit was allocated. Thus, the net revenue gain for a true negative is \$0.

-   False Positive - Predicted incorrectly homeowner would enter the credit program; allocated marketing resources; no credit allocated. We count a net revenue loss of \$2,850 per false positive.

-   False Negative - We predicted that a homeowner would not enter the credit program but they did. These are likely homeowners who signed up for reasons unrelated to the marketing campaign. Thus, the net revenue gain for a false negative is \$0.

Because the benefits of a true positive significantly outweigh the cost of a false positive (we earn \$12,400 per true positive, while only losing \$2,850 per false positive), we want to optimize our model to optimize sensitivity (true positive rate), rather than balanced accuracy.


```{r random outreach}
rand_out <- housing_subsidy_data %>%
  group_by(y) %>%
  summarize(count = n()) %>%
  mutate(revenue = case_when(
    y == "no" ~ 0.5 * count * -2850,
    TRUE ~ 0.5 * count * 12400
  ))

print(sum(rand_out$revenue))

```

Write out the cost/benefit equation for each confusion metric.

Create the 'Cost/Benefit Table' as seen above.

Plot the confusion metric outcomes for each Threshold.

Create two small multiple plots that show Threshold as a function of Total_Revenue and Total_Count_of_Credits. Interpret this. Create a table of the Total_Revenue and Total_Count_of_Credits allocated for 2 categories. 50%\_Threshold and your Optimal_Threshold.

## Discussion

- Importance of feature engineering
- Importance of recognizing what is being optimized for (accuracy versus revenue); differences between modeling versus real life
- limits to the power of a logit model; something like a lasso regression, or maybe even a random forest model, might be better here
- One possibility is that this model would perform substantially better with more data. 

We will also need a larger sample size than in OLS because in logistic regression we rely on maximum likelihood estimation.

- Rare event modeling in logistic regression: 

"To judge whether logistic regression is appropriate here, we need to examine the number of cases, especially the number of the rarer outcome of the two – in this cases, drivers that were drunk at the time of crashes are a lot less than those who were not drunk. If there is a small sample bias here, we can use Paul Allison proposed methods for modeling rare events, similar to penalized likelihood, for reducing small-sample biases in Maximum likelihood estimation.

The question is then whether the number of drunk drivers should be considered small sample. The answer is uncertain. According to Table 1, we have 2,485 observations for drivers who were drunk at the time of incidents, versus 40,879 observations for drivers who were not drunk. Although 2,485 seems like a reasonably sized group, it only take 5.7% of all observations. Therefore, there might be a need to adopt Paul Allison’s methods for modeling rare events instead of just using logistic regression."


## Conclusion

All this model has to do is be better than a coin flip, basically, so it's a low bar. But this is a common occurance in city government, in which SoP basically is no better than random outreach, and even a small amount of modelling can have enormous RoI.

Based on the exploratory data analysis, I would target outreach to specific times of the year. I might even have a trigger based on things like the consumer price index to up my outreach during those periods.

Conclude whether and why this model should or shouldn't be put into production. What could make the model better? What would you do to ensure that the marketing materials resulted in a better response rate?
